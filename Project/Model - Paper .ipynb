{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path = \"data/\"\n",
    "mitbih_train = pd.read_csv(path+\"mitbih_train.csv\")\n",
    "mitbih_train.columns = list(range(len(mitbih_train.columns)))\n",
    "mitbih_test = pd.read_csv(path+\"mitbih_test.csv\")\n",
    "mitbih_test.columns = list(range(len(mitbih_test.columns)))\n",
    "mitbih_train[187] = mitbih_train[187].astype(int)\n",
    "mitbih_test[187] = mitbih_test[187].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    72470\n",
       "4     6431\n",
       "2     5788\n",
       "1     2223\n",
       "3      641\n",
       "Name: 187, dtype: int64"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mitbih_train[187].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "2000\n",
      "8641\n"
     ]
    }
   ],
   "source": [
    "# print(len(mitbih_train[mitbih_train[187]==0]))\n",
    "print(len(mitbih_train[mitbih_train[187]==1]))\n",
    "mitbih_train0 = mitbih_train[mitbih_train[187]==0]\n",
    "mitbih_train1 = mitbih_train[mitbih_train[187]==1]\n",
    "\n",
    "mitbih_train2 = mitbih_train[mitbih_train[187]==2]\n",
    "mitbih_train4 = mitbih_train[mitbih_train[187]==4]\n",
    "\n",
    "\n",
    "drop_indices = np.random.choice(mitbih_train0.index, len(mitbih_train0)-2000, replace=False)\n",
    "mitbih_train = mitbih_train.drop(drop_indices)\n",
    "drop_indices = np.random.choice(mitbih_train2.index, len(mitbih_train2)-2000, replace=False)\n",
    "mitbih_train = mitbih_train.drop(drop_indices)\n",
    "drop_indices = np.random.choice(mitbih_train4.index, len(mitbih_train4)-2000, replace=False)\n",
    "mitbih_train = mitbih_train.drop(drop_indices)\n",
    "drop_indices = np.random.choice(mitbih_train1.index, len(mitbih_train1)-2000, replace=False)\n",
    "mitbih_train = mitbih_train.drop(drop_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(mitbih_train[mitbih_train[187]==0]))\n",
    "print(len(mitbih_train[mitbih_train[187]==1]))\n",
    "print(len(mitbih_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = mitbih_train.drop([187], axis=1)\n",
    "trainy = mitbih_train[187]\n",
    "testx = mitbih_test.drop([187], axis=1)\n",
    "testy = mitbih_test[187]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    2000\n",
      "2    2000\n",
      "1    2000\n",
      "0    2000\n",
      "3     641\n",
      "Name: 187, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjaydave/.pyenv/versions/3.7.4/lib/python3.7/site-packages/imblearn/utils/_validation.py:638: FutureWarning: Pass sampling_strategy=not majority as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    2000\n",
      "3    2000\n",
      "2    2000\n",
      "1    2000\n",
      "0    2000\n",
      "Name: 187, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resample the dataset to have equal number of examples for each class. Data resampling using sklearn.\n",
    "Data augmentation technique changes the value of signal, we are not sure which augmentation technique won't\n",
    "change the prediction of ECG signal.\n",
    "\"\"\"\n",
    "print(trainy.value_counts())\n",
    "sma = SMOTE('not majority', random_state=40)\n",
    "trainx, trainy = sma.fit_resample(trainx, trainy)\n",
    "print(trainy.value_counts())\n",
    "trainx = np.expand_dims(trainx, 2)\n",
    "testx = np.expand_dims(testx, 2)\n",
    "\n",
    "# ohe = OneHotEncoder()\n",
    "# testy = ohe.fit_transform(np.array(testy).reshape(-1,1))\n",
    "# trainy = ohe.transform(np.array(trainy).reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 187, 1)\n",
      "(10000,)\n",
      "(21891, 187, 1)\n",
      "(21891,)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)\n",
    "trainn, dim, _ = trainx.shape\n",
    "testn = testy.shape[0]\n",
    "tags = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainx.columns\n",
    "#trainx_tensors = [torch.tensor(np.array(df)) for df in trainx]\n",
    "#torch.stack(list_of_tensors)\n",
    "#print(trainx_tensors[0].size())\n",
    "#tensor0 = torch.tensor()\n",
    "#tensor1 = torch.tensor(np.array(trainx.iloc[1]))\n",
    "# tensor1 = torch.tensor([[np.array(trainx.iloc[0])]])\n",
    "# tensor2 = torch.tensor([[np.array(trainx.iloc[1])]])\n",
    "# tensor.size()\n",
    "# tensory = torch.tensor(trainy)\n",
    "\n",
    "ttrainx = torch.tensor([np.array(trainx[i]).reshape(1,187) for i in range(trainn)])\n",
    "ttestx = torch.tensor([np.array(testx[i]).reshape(1,187) for i in range(testn)])\n",
    "ttesty = torch.tensor(testy)\n",
    "ttrainy = torch.tensor(trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 187])\n",
      "torch.Size([10000])\n",
      "torch.Size([21891, 1, 187])\n",
      "torch.Size([21891])\n"
     ]
    }
   ],
   "source": [
    "print(ttrainx.shape)\n",
    "print(ttrainy.shape)\n",
    "print(ttestx.shape)\n",
    "print(ttesty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(len(trainy))\n",
    "# one_hot = torch.zeros(testn, tags)\n",
    "# one_hot[torch.arange(testn), ttesty] = 1\n",
    "# ttesty = one_hot\n",
    "\n",
    "# one_hot = torch.zeros(trainn, tags)\n",
    "# one_hot[torch.arange(trainn), ttrainy] = 1\n",
    "# ttrainy = one_hot\n",
    "\n",
    "\n",
    "# #ttesty = torch.tensor([np.array(testy[i]) for i in range(testn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21891, 5]) torch.Size([362350, 5])\n"
     ]
    }
   ],
   "source": [
    "# print(ttesty.size(), ttrainy.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "train = data_utils.TensorDataset(ttrainx, ttrainy)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=500, shuffle=True)\n",
    "\n",
    "\n",
    "test = data_utils.TensorDataset(ttestx, ttesty)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=100, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 187]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# # get some random training images\n",
    "# dataiter = iter(train_loader)\n",
    "# x, labels = dataiter.next()\n",
    "# print(x.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model described in paper\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5)\n",
    "\n",
    "        self.conv11 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "        self.conv12 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        self.conv21 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "        self.conv22 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        self.conv31 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "        self.conv32 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        self.conv41 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "        self.conv42 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        self.conv51 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "        self.conv52 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        self.flatten1 = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        \n",
    "    def do_next(self, x, conv1, conv2):\n",
    "        x1 = x\n",
    "        x = conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = conv2(x)\n",
    "        x = torch.add(x1, x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=5, stride=2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.do_next(x, self.conv11, self.conv12)\n",
    "        x = self.do_next(x, self.conv21, self.conv22)\n",
    "        x = self.do_next(x, self.conv31, self.conv32)\n",
    "        x = self.do_next(x, self.conv41, self.conv42)\n",
    "        x = self.do_next(x, self.conv51, self.conv52)\n",
    "        x = self.flatten1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveNet(Net):\n",
    "    def __init__(self):\n",
    "        super(FiveNet, self).__init__()\n",
    "        self.fc3 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = super(FiveNet, self).forward(inp)\n",
    "        #print(x)\n",
    "        x = self.fc3(x)\n",
    "        #output = F.softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "fivenet = FiveNet()\n",
    "# adam = optim.Adam(fivenet.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "best_model_wts = copy.deepcopy(my_nn.state_dict())\n",
    "fivenet.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 21891 test inputs: 86.3140103238774 %%\n",
      "86.3140103238774\n"
     ]
    }
   ],
   "source": [
    "print(model_test(fivenet, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5339e-01,  1.2765e-01, -1.0228e-01, -1.4452e-02,  9.3743e-02],\n",
      "        [-1.3959e-01,  1.2678e-01, -1.1014e-01,  4.2389e-04,  7.2181e-02],\n",
      "        [-1.3961e-01,  1.3812e-01, -9.8849e-02,  9.6237e-05,  1.0807e-01],\n",
      "        [-1.4908e-01,  1.3826e-01, -8.9222e-02, -1.3549e-02,  1.0736e-01],\n",
      "        [-1.5443e-01,  1.3843e-01, -8.9461e-02, -2.1898e-03,  9.8225e-02],\n",
      "        [-1.3709e-01,  1.5392e-01, -8.2855e-02, -1.5285e-03,  1.0699e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "my_nn = FiveNet()\n",
    "#print(trainx[0].shape)\n",
    "#print(tensor.size())\n",
    "p = my_nn(tensor.float())\n",
    "#p = my_nn(torch.tensor(np.array(trainx[0])).float())\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1132, -0.2509, -2.4716,  4.5064,  2.1217,  1.0548, -1.3551, -1.3960,\n",
      "         -2.4039,  0.5018, -1.0359, -1.0365, -4.3443,  0.6044,  3.2484,  0.7197,\n",
      "          4.7650,  7.8272,  2.4695,  5.8419, -2.1150, -1.4809,  2.4006,  0.5666,\n",
      "         -1.8740, -3.7884,  0.1279, -2.1668, -2.4403, -5.8466,  2.8172,  4.3611],\n",
      "        [ 1.5620, -3.8722, -0.7775,  0.3315,  3.9262,  2.4394, -5.0029, -3.4208,\n",
      "         -3.5437,  1.6339, -2.8805, -0.4099, -5.9477,  2.9163,  3.8448,  1.1345,\n",
      "          4.6027,  7.1737,  0.6896,  3.7839,  1.2704, -0.3600,  2.4920, -1.6533,\n",
      "          0.4535, -3.8329, -0.4199, -0.0648, -4.6679, -7.8758,  3.5919,  2.5132],\n",
      "        [ 2.0435, -3.3133, -1.7740,  1.4034,  2.8841,  0.4741, -4.7121, -4.3640,\n",
      "         -1.2834,  1.1913, -0.9667, -0.6064, -6.4064,  2.1131,  4.4597,  1.9161,\n",
      "          4.4311,  4.9684,  0.9270,  3.0831,  1.1098, -0.3981,  2.5687, -2.2672,\n",
      "         -1.1763, -4.7349,  0.3055, -0.2109, -3.8875, -6.1572,  2.1901,  0.3564],\n",
      "        [ 0.0458,  0.2767, -2.4268,  4.8054,  1.8449,  0.4955, -1.0114, -1.1871,\n",
      "         -1.8361,  0.1560,  0.1801, -0.7901, -4.2808,  0.1929,  3.2727,  0.7332,\n",
      "          4.4877,  7.2757,  2.6274,  5.7651, -2.2337, -1.5369,  2.4704,  0.5271,\n",
      "         -2.8427, -4.3429,  0.1525, -1.9103, -2.3021, -5.1842,  2.2421,  3.8477],\n",
      "        [ 0.1467,  1.3877, -1.1231,  4.3520,  2.6165,  0.7829,  0.2138, -0.6436,\n",
      "         -1.1194, -0.4983,  0.7884, -0.0899, -1.3661,  1.2508,  1.9640,  0.5124,\n",
      "          3.7313,  6.0674,  2.0308,  4.9707, -3.2733, -2.4352,  3.8734,  1.2350,\n",
      "         -3.7326, -4.0958, -0.2254, -1.7711, -2.7148, -4.0471,  1.9108,  2.7404],\n",
      "        [ 1.7421,  1.2741,  0.7373,  2.2307,  2.3325,  0.6713,  0.0695, -0.7662,\n",
      "         -1.0030, -1.5009,  1.1347,  0.2478, -0.2755,  2.3727,  0.7607,  0.1678,\n",
      "          3.0239,  2.9082,  0.1527,  2.1905, -1.9156, -0.5433,  3.2475,  1.9455,\n",
      "         -3.9719, -3.2840, -1.1064,  0.2578, -3.1558, -2.1519,  1.0519,  1.1794]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5913, -3.1482],\n",
      "        [ 3.3957, -4.7922],\n",
      "        [ 2.7737, -4.4248],\n",
      "        [ 1.1961, -2.8574],\n",
      "        [ 0.5693, -1.5503],\n",
      "        [ 0.4623, -1.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1132, -0.2509, -2.4716,  4.5064,  2.1217,  1.0548, -1.3551, -1.3960,\n",
      "         -2.4039,  0.5018, -1.0359, -1.0365, -4.3443,  0.6044,  3.2484,  0.7197,\n",
      "          4.7650,  7.8272,  2.4695,  5.8419, -2.1150, -1.4809,  2.4006,  0.5666,\n",
      "         -1.8740, -3.7884,  0.1279, -2.1668, -2.4403, -5.8466,  2.8172,  4.3611],\n",
      "        [ 1.5620, -3.8722, -0.7775,  0.3315,  3.9262,  2.4394, -5.0029, -3.4208,\n",
      "         -3.5437,  1.6339, -2.8805, -0.4099, -5.9477,  2.9163,  3.8448,  1.1345,\n",
      "          4.6027,  7.1737,  0.6896,  3.7839,  1.2704, -0.3600,  2.4920, -1.6533,\n",
      "          0.4535, -3.8329, -0.4199, -0.0648, -4.6679, -7.8758,  3.5919,  2.5132],\n",
      "        [ 2.0435, -3.3133, -1.7740,  1.4034,  2.8841,  0.4741, -4.7121, -4.3640,\n",
      "         -1.2834,  1.1913, -0.9667, -0.6064, -6.4064,  2.1131,  4.4597,  1.9161,\n",
      "          4.4311,  4.9684,  0.9270,  3.0831,  1.1098, -0.3981,  2.5687, -2.2672,\n",
      "         -1.1763, -4.7349,  0.3055, -0.2109, -3.8875, -6.1572,  2.1901,  0.3564],\n",
      "        [ 0.0458,  0.2767, -2.4268,  4.8054,  1.8449,  0.4955, -1.0114, -1.1871,\n",
      "         -1.8361,  0.1560,  0.1801, -0.7901, -4.2808,  0.1929,  3.2727,  0.7332,\n",
      "          4.4877,  7.2757,  2.6274,  5.7651, -2.2337, -1.5369,  2.4704,  0.5271,\n",
      "         -2.8427, -4.3429,  0.1525, -1.9103, -2.3021, -5.1842,  2.2421,  3.8477],\n",
      "        [ 0.1467,  1.3877, -1.1231,  4.3520,  2.6165,  0.7829,  0.2138, -0.6436,\n",
      "         -1.1194, -0.4983,  0.7884, -0.0899, -1.3661,  1.2508,  1.9640,  0.5124,\n",
      "          3.7313,  6.0674,  2.0308,  4.9707, -3.2733, -2.4352,  3.8734,  1.2350,\n",
      "         -3.7326, -4.0958, -0.2254, -1.7711, -2.7148, -4.0471,  1.9108,  2.7404],\n",
      "        [ 1.7421,  1.2741,  0.7373,  2.2307,  2.3325,  0.6713,  0.0695, -0.7662,\n",
      "         -1.0030, -1.5009,  1.1347,  0.2478, -0.2755,  2.3727,  0.7607,  0.1678,\n",
      "          3.0239,  2.9082,  0.1527,  2.1905, -1.9156, -0.5433,  3.2475,  1.9455,\n",
      "         -3.9719, -3.2840, -1.1064,  0.2578, -3.1558, -2.1519,  1.0519,  1.1794]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([[ 4.2848,  4.0665, -1.4479, -3.3159, -0.5536],\n",
      "        [ 4.1265,  3.6876, -2.9989,  2.0729, -5.3447],\n",
      "        [ 3.4043,  0.4699, -1.9301,  1.8831, -2.8954],\n",
      "        [ 4.0134,  3.1517, -0.9091, -3.7152,  0.0816],\n",
      "        [ 2.6926,  3.2749,  0.5531, -4.3031,  0.6538],\n",
      "        [ 0.7201,  1.6657,  2.0275, -2.1404, -0.8681]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "p = twonn(tensor.float())\n",
    "#p = my_nn(torch.tensor(np.array(trainx[0])).float())\n",
    "\n",
    "print(p)\n",
    "p = my_nn(tensor.float())\n",
    "#p = my_nn(torch.tensor(np.array(trainx[0])).float())\n",
    "\n",
    "print(p)\n",
    "\n",
    "# p = my_nn(tensor2.float())\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(net, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy test inputs: {} %%'.format(100 * correct / total))\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = FiveNet()\n",
    "\n",
    "adam = optim.Adam(my_nn.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 100\n",
    "loss = 0.4\n",
    "#trainn/500\n",
    "training_loss = []\n",
    "test_acc = []\n",
    "running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.000\n",
      "Accuracy test inputs: 7.345484445662601 %%\n",
      "[2] loss: 31.024\n",
      "Accuracy test inputs: 24.4895162395505 %%\n",
      "[3] loss: 24.726\n",
      "Accuracy test inputs: 62.82490521218766 %%\n",
      "[4] loss: 19.943\n",
      "Accuracy test inputs: 51.07121648165913 %%\n",
      "[5] loss: 16.153\n",
      "Accuracy test inputs: 62.26303046914257 %%\n",
      "[6] loss: 13.342\n",
      "Accuracy test inputs: 64.7663423324654 %%\n",
      "[7] loss: 11.877\n",
      "Accuracy test inputs: 78.1919510301037 %%\n",
      "[8] loss: 10.716\n",
      "Accuracy test inputs: 73.69238499840117 %%\n",
      "[9] loss: 9.990\n",
      "Accuracy test inputs: 84.55529669727285 %%\n",
      "[10] loss: 9.727\n",
      "Accuracy test inputs: 63.10355854003929 %%\n",
      "[11] loss: 8.689\n",
      "Accuracy test inputs: 77.78995934402266 %%\n",
      "[12] loss: 8.411\n",
      "Accuracy test inputs: 87.02206386186104 %%\n",
      "[13] loss: 8.083\n",
      "Accuracy test inputs: 77.14129094148281 %%\n",
      "[14] loss: 8.093\n",
      "Accuracy test inputs: 72.40875245534694 %%\n",
      "[15] loss: 7.326\n",
      "Accuracy test inputs: 82.74633410990818 %%\n",
      "[16] loss: 7.135\n",
      "Accuracy test inputs: 69.2933168882189 %%\n",
      "[17] loss: 6.933\n",
      "Accuracy test inputs: 89.69896304417341 %%\n",
      "[18] loss: 6.633\n",
      "Accuracy test inputs: 77.76255081997168 %%\n",
      "[19] loss: 6.259\n",
      "Accuracy test inputs: 81.18861632634416 %%\n",
      "[20] loss: 6.031\n",
      "Accuracy test inputs: 89.46599058974007 %%\n",
      "[21] loss: 5.940\n",
      "Accuracy test inputs: 74.07610433511489 %%\n",
      "[22] loss: 6.077\n",
      "Accuracy test inputs: 92.63624320497009 %%\n",
      "[23] loss: 6.014\n",
      "Accuracy test inputs: 77.61637202503312 %%\n",
      "[24] loss: 5.979\n",
      "Accuracy test inputs: 88.30112831757343 %%\n",
      "[25] loss: 5.678\n",
      "Accuracy test inputs: 84.96642455803755 %%\n",
      "[26] loss: 5.150\n",
      "Accuracy test inputs: 81.46726965419579 %%\n",
      "[27] loss: 4.996\n",
      "Accuracy test inputs: 79.63089854278014 %%\n",
      "[28] loss: 5.402\n",
      "Accuracy test inputs: 92.18400255812891 %%\n",
      "[29] loss: 5.121\n",
      "Accuracy test inputs: 82.3534785985108 %%\n",
      "[30] loss: 4.587\n",
      "Accuracy test inputs: 86.18610387830616 %%\n",
      "[31] loss: 4.392\n",
      "Accuracy test inputs: 84.61011374537482 %%\n",
      "[32] loss: 4.353\n",
      "Accuracy test inputs: 91.19272760495181 %%\n",
      "[33] loss: 4.139\n",
      "Accuracy test inputs: 88.14581334795122 %%\n",
      "[34] loss: 4.353\n",
      "Accuracy test inputs: 80.68155863140103 %%\n",
      "[35] loss: 4.062\n",
      "Accuracy test inputs: 90.4298570188662 %%\n",
      "[36] loss: 3.845\n",
      "Accuracy test inputs: 86.77538714540222 %%\n",
      "[37] loss: 4.029\n",
      "Accuracy test inputs: 91.04654881001325 %%\n",
      "[38] loss: 4.016\n",
      "Accuracy test inputs: 88.60719016947604 %%\n",
      "[39] loss: 3.697\n",
      "Accuracy test inputs: 84.95728838335388 %%\n",
      "[40] loss: 3.520\n",
      "Accuracy test inputs: 88.70312000365448 %%\n",
      "[41] loss: 3.502\n",
      "Accuracy test inputs: 88.10926864921657 %%\n",
      "[42] loss: 3.277\n",
      "Accuracy test inputs: 85.88004202640354 %%\n",
      "[43] loss: 3.556\n",
      "Accuracy test inputs: 92.89662418345439 %%\n",
      "[44] loss: 3.273\n",
      "Accuracy test inputs: 87.01292768717738 %%\n",
      "[45] loss: 3.481\n",
      "Accuracy test inputs: 93.53615641131059 %%\n",
      "[46] loss: 3.544\n",
      "Accuracy test inputs: 91.1013658581152 %%\n",
      "[47] loss: 3.200\n",
      "Accuracy test inputs: 86.42364442008132 %%\n",
      "[48] loss: 3.055\n",
      "Accuracy test inputs: 89.58476086062765 %%\n",
      "[49] loss: 3.044\n",
      "Accuracy test inputs: 90.24256543785117 %%\n",
      "[50] loss: 2.847\n",
      "Accuracy test inputs: 90.79987209355443 %%\n",
      "[51] loss: 2.764\n",
      "Accuracy test inputs: 89.3015394454342 %%\n",
      "[52] loss: 2.671\n",
      "Accuracy test inputs: 93.417386140423 %%\n",
      "[53] loss: 2.842\n",
      "Accuracy test inputs: 93.26207117080078 %%\n",
      "[54] loss: 2.580\n",
      "Accuracy test inputs: 88.31483257959893 %%\n",
      "[55] loss: 2.677\n",
      "Accuracy test inputs: 80.59476497190626 %%\n",
      "[56] loss: 2.563\n",
      "Accuracy test inputs: 91.38915536065049 %%\n",
      "[57] loss: 2.379\n",
      "Accuracy test inputs: 93.79653738979489 %%\n",
      "[58] loss: 2.528\n",
      "Accuracy test inputs: 89.60303320999498 %%\n",
      "[59] loss: 2.172\n",
      "Accuracy test inputs: 87.04947238591203 %%\n",
      "[60] loss: 2.208\n",
      "Accuracy test inputs: 91.90078114293546 %%\n",
      "[61] loss: 2.049\n",
      "Accuracy test inputs: 85.08976291626696 %%\n",
      "[62] loss: 2.546\n",
      "Accuracy test inputs: 88.447307112512 %%\n",
      "[63] loss: 2.408\n",
      "Accuracy test inputs: 87.72554931250285 %%\n",
      "[64] loss: 2.112\n",
      "Accuracy test inputs: 89.40660545429628 %%\n",
      "[65] loss: 1.925\n",
      "Accuracy test inputs: 87.81234297199762 %%\n",
      "[66] loss: 2.003\n",
      "Accuracy test inputs: 93.00169019231647 %%\n",
      "[67] loss: 2.024\n",
      "Accuracy test inputs: 86.17239961628066 %%\n",
      "[68] loss: 1.653\n",
      "Accuracy test inputs: 89.98675254670869 %%\n",
      "[69] loss: 1.573\n",
      "Accuracy test inputs: 89.32894796948517 %%\n",
      "[70] loss: 2.102\n",
      "Accuracy test inputs: 86.6337764378055 %%\n",
      "[71] loss: 2.171\n",
      "Accuracy test inputs: 87.27787675300351 %%\n",
      "[72] loss: 1.561\n",
      "Accuracy test inputs: 86.06276552007674 %%\n",
      "[73] loss: 1.440\n",
      "Accuracy test inputs: 90.9780274998858 %%\n",
      "[74] loss: 1.528\n",
      "Accuracy test inputs: 84.75172445297154 %%\n",
      "[75] loss: 2.490\n",
      "Accuracy test inputs: 85.15371613905258 %%\n",
      "[76] loss: 1.753\n",
      "Accuracy test inputs: 92.3804303138276 %%\n",
      "[77] loss: 1.585\n",
      "Accuracy test inputs: 90.65369329861586 %%\n",
      "[78] loss: 1.750\n",
      "Accuracy test inputs: 86.55611895299438 %%\n",
      "[79] loss: 1.335\n",
      "Accuracy test inputs: 89.35635649353615 %%\n",
      "[80] loss: 1.193\n",
      "Accuracy test inputs: 91.7546023479969 %%\n",
      "[81] loss: 1.149\n",
      "Accuracy test inputs: 92.96514549358184 %%\n",
      "[82] loss: 1.421\n",
      "Accuracy test inputs: 91.18359143026815 %%\n",
      "[83] loss: 1.442\n",
      "Accuracy test inputs: 85.51459503905714 %%\n",
      "[84] loss: 1.478\n",
      "Accuracy test inputs: 92.8372390480106 %%\n",
      "[85] loss: 1.260\n",
      "Accuracy test inputs: 86.50130190489241 %%\n",
      "[86] loss: 1.269\n",
      "Accuracy test inputs: 89.17363299986296 %%\n",
      "[87] loss: 1.177\n",
      "Accuracy test inputs: 90.88209766570736 %%\n",
      "[88] loss: 1.461\n",
      "Accuracy test inputs: 91.41199579735965 %%\n",
      "[89] loss: 1.186\n",
      "Accuracy test inputs: 88.662007217578 %%\n",
      "[90] loss: 0.881\n",
      "Accuracy test inputs: 91.84139600749167 %%\n",
      "[91] loss: 0.782\n",
      "Accuracy test inputs: 80.11511580101411 %%\n",
      "[92] loss: 1.516\n",
      "Accuracy test inputs: 79.69485176556576 %%\n",
      "[93] loss: 1.108\n",
      "Accuracy test inputs: 92.30734091635833 %%\n",
      "[94] loss: 1.066\n",
      "Accuracy test inputs: 88.94979672011328 %%\n",
      "[95] loss: 0.901\n",
      "Accuracy test inputs: 90.24256543785117 %%\n",
      "[96] loss: 1.078\n",
      "Accuracy test inputs: 90.2014526517747 %%\n",
      "[97] loss: 0.949\n",
      "Accuracy test inputs: 89.55278424923485 %%\n",
      "[98] loss: 0.900\n",
      "Accuracy test inputs: 92.63624320497009 %%\n",
      "[99] loss: 0.775\n",
      "Accuracy test inputs: 89.8816865378466 %%\n",
      "[100] loss: 0.616\n",
      "Accuracy test inputs: 90.19688456443288 %%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(100):\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (e + 1, running_loss ))\n",
    "    tacc = model_test(my_nn, test_loader)\n",
    "    training_loss.append(running_loss )\n",
    "    test_acc.append(tacc)\n",
    "    running_loss = 0.0\n",
    "    #print(test_acc)\n",
    "    if test_acc[-1] > 90:\n",
    "        print(\"early stopping! Accuracy on test dataset > 90%\")\n",
    "        break\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        #print(type(i), type(data))\n",
    "        inputs, labels = data\n",
    "        #print(inputs.size(), labels.size())\n",
    "        adam.zero_grad()\n",
    "        outputs = my_nn(inputs.float())\n",
    "        #print(outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        running_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(my_nn.state_dict(), \"model/model_87\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_nn = FiveNet()\n",
    "path = \"model/model_91\"\n",
    "my_nn.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18117\n",
      "4     1608\n",
      "2     1448\n",
      "1      556\n",
      "3      162\n",
      "Name: 187, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(testy.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0    4045\n",
      "1    10505\n",
      "Name: 187, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "0    4045\n",
    "1    10505\n",
    "Name: 187, dtype: int64\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test inputs: 91.00543602393678 %%\n"
     ]
    }
   ],
   "source": [
    "#tacc = model_test(my_nn, test_loader)\n",
    "#print(tacc)\n",
    "from sklearn.metrics import classification_report\n",
    "allpredicted = []\n",
    "acc2 = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = my_nn(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        allpredicted += predicted\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy test inputs: {} %%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81     17409\n",
      "           1       0.06      0.03      0.04      1066\n",
      "           2       0.07      0.07      0.07      1520\n",
      "           3       0.02      0.01      0.01       307\n",
      "           4       0.07      0.08      0.08      1589\n",
      "\n",
      "    accuracy                           0.67     21891\n",
      "   macro avg       0.20      0.20      0.20     21891\n",
      "weighted avg       0.65      0.67      0.66     21891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(allpredicted, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07251381215469613\n"
     ]
    }
   ],
   "source": [
    "testy2 =testy[testy==2]\n",
    "a2 = np.array(allpredicted)[testy==2]\n",
    "print(sum(testy2==a2)/1448)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "path = \"data/\"\n",
    "ptbdb_abnormal_train = pd.read_csv(path+\"ptbdb_abnormal.csv\")\n",
    "ptbdb_normal_train = pd.read_csv(path+\"ptbdb_normal.csv\")\n",
    "ptbdb_abnormal_train.columns = list(range(len(ptbdb_abnormal_train.columns)))\n",
    "ptbdb_normal_train.columns = list(range(len(ptbdb_normal_train.columns)))\n",
    "\n",
    "ptbdb_abnormal_train[187] = ptbdb_abnormal_train[187].astype(int)\n",
    "ptbdb_normal_train[187] = ptbdb_normal_train[187].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4045, 188)\n",
      "(10505, 188)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(ptbdb_normal_train.shape)\n",
    "print(ptbdb_abnormal_train.shape)\n",
    "normalx = ptbdb_normal_train.drop([187], axis=1)\n",
    "normaly = ptbdb_normal_train[187]\n",
    "abnormalx = ptbdb_abnormal_train.drop([187], axis=1)\n",
    "abnormaly = ptbdb_abnormal_train[187]\n",
    "\n",
    "ptbdb_allx = np.concatenate((normalx, abnormalx), axis=0)\n",
    "ptbdb_ally = np.concatenate((normaly, abnormaly), axis=0)\n",
    "\n",
    "ptrainx, ptestx, ptrainy, ptesty = train_test_split(ptbdb_allx, ptbdb_ally, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4045, 188)\n",
      "(10505, 188)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(ptbdb_normal_train.shape)\n",
    "print(ptbdb_abnormal_train.shape)\n",
    "normalx = ptbdb_normal_train.drop([187], axis=1)\n",
    "normaly = ptbdb_normal_train[187]\n",
    "abnormalx = ptbdb_abnormal_train.drop([187], axis=1)\n",
    "abnormaly = ptbdb_abnormal_train[187]\n",
    "\n",
    "ptbdb_allx = np.concatenate((normalx, abnormalx), axis=0)\n",
    "ptbdb_ally = np.concatenate((normaly, abnormaly), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([3216, 8424]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(ptrainy, return_counts=True))\n",
    "sma = SMOTE('not majority', random_state=40)\n",
    "ptrainx, ptrainy = sma.fit_resample(ptrainx, ptrainy)\n",
    "ptrainx = np.expand_dims(ptrainx, 2)\n",
    "ptestx = np.expand_dims(ptestx, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([8424, 8424]))\n",
      "(16848, 187, 1)\n",
      "(16848,)\n",
      "(2910, 187, 1)\n",
      "(2910,)\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(ptrainy, return_counts=True))\n",
    "print(ptrainx.shape)\n",
    "print(ptrainy.shape)\n",
    "print(ptestx.shape)\n",
    "print(ptesty.shape)\n",
    "ptrainn, dim, _ = ptrainx.shape\n",
    "ptestn = ptesty.shape[0]\n",
    "tags = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "pttrainx = torch.tensor([np.array(ptrainx[i]).reshape(1,187) for i in range(ptrainn)])\n",
    "pttestx = torch.tensor([np.array(ptestx[i]).reshape(1,187) for i in range(ptestn)])\n",
    "pttesty = torch.tensor(ptesty)\n",
    "pttrainy = torch.tensor(ptrainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrain = data_utils.TensorDataset(pttrainx, pttrainy)\n",
    "ptrain_loader = data_utils.DataLoader(ptrain, batch_size=500, shuffle=True)\n",
    "\n",
    "ptest = data_utils.TensorDataset(pttestx, pttesty)\n",
    "ptest_loader = data_utils.DataLoader(ptest, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoNet(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super(TwoNet, self).__init__()\n",
    "        self.net = net\n",
    "        self.net.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        #x = super(FiveNet, self.net).forward(inp)\n",
    "        x = self.net.forward(inp)\n",
    "        #print(x)\n",
    "        #x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True]\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([i.requires_grad for i in my_nn.parameters()])\n",
    "twonn = TwoNet(my_nn)\n",
    "print([i.requires_grad for i in twonn.parameters()])\n",
    "adam = optim.Adam(filter(lambda p: p.requires_grad, twonn.parameters()), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 100\n",
    "loss = 0.4\n",
    "#trainn/500\n",
    "training_loss = []\n",
    "test_acc = []\n",
    "running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True]\n",
      "[1] loss: 0.000\n",
      "Accuracy test inputs: 80.10309278350516 %%\n",
      "[2] loss: 192.580\n",
      "Accuracy test inputs: 78.21305841924399 %%\n",
      "[3] loss: 299.057\n",
      "Accuracy test inputs: 75.53264604810997 %%\n",
      "[4] loss: 398.564\n",
      "Accuracy test inputs: 74.5704467353952 %%\n",
      "[5] loss: 443.175\n",
      "Accuracy test inputs: 74.5704467353952 %%\n",
      "[6] loss: 415.536\n",
      "Accuracy test inputs: 75.25773195876289 %%\n",
      "[7] loss: 323.567\n",
      "Accuracy test inputs: 78.00687285223368 %%\n",
      "[8] loss: 210.440\n",
      "Accuracy test inputs: 80.44673539518901 %%\n",
      "[9] loss: 137.443\n",
      "Accuracy test inputs: 78.14432989690722 %%\n",
      "[10] loss: 137.401\n",
      "Accuracy test inputs: 72.98969072164948 %%\n",
      "[11] loss: 192.464\n",
      "Accuracy test inputs: 67.42268041237114 %%\n",
      "[12] loss: 269.104\n",
      "Accuracy test inputs: 62.61168384879725 %%\n",
      "[13] loss: 346.585\n",
      "Accuracy test inputs: 58.76288659793814 %%\n",
      "[14] loss: 413.769\n",
      "Accuracy test inputs: 55.56701030927835 %%\n",
      "[15] loss: 454.923\n",
      "Accuracy test inputs: 54.15807560137457 %%\n",
      "[16] loss: 467.240\n",
      "Accuracy test inputs: 54.5360824742268 %%\n",
      "[17] loss: 446.480\n",
      "Accuracy test inputs: 56.94158075601374 %%\n",
      "[18] loss: 395.368\n",
      "Accuracy test inputs: 60.65292096219931 %%\n",
      "[19] loss: 323.209\n",
      "Accuracy test inputs: 65.32646048109966 %%\n",
      "[20] loss: 246.207\n",
      "Accuracy test inputs: 70.37800687285224 %%\n",
      "[21] loss: 176.158\n",
      "Accuracy test inputs: 75.4295532646048 %%\n",
      "[22] loss: 137.203\n",
      "Accuracy test inputs: 79.17525773195877 %%\n",
      "[23] loss: 151.242\n",
      "Accuracy test inputs: 80.61855670103093 %%\n",
      "[24] loss: 221.972\n",
      "Accuracy test inputs: 78.62542955326461 %%\n",
      "[25] loss: 326.473\n",
      "Accuracy test inputs: 76.01374570446735 %%\n",
      "[26] loss: 422.144\n",
      "Accuracy test inputs: 74.3298969072165 %%\n",
      "[27] loss: 475.422\n",
      "Accuracy test inputs: 73.98625429553265 %%\n",
      "[28] loss: 468.800\n",
      "Accuracy test inputs: 74.7766323024055 %%\n",
      "[29] loss: 402.049\n",
      "Accuracy test inputs: 76.76975945017182 %%\n",
      "[30] loss: 299.534\n",
      "Accuracy test inputs: 79.72508591065292 %%\n",
      "[31] loss: 201.881\n",
      "Accuracy test inputs: 81.1340206185567 %%\n",
      "[32] loss: 150.716\n",
      "Accuracy test inputs: 79.34707903780068 %%\n",
      "[33] loss: 154.138\n",
      "Accuracy test inputs: 74.63917525773196 %%\n",
      "[34] loss: 201.492\n",
      "Accuracy test inputs: 71.06529209621993 %%\n",
      "[35] loss: 266.995\n",
      "Accuracy test inputs: 66.52920962199313 %%\n",
      "[36] loss: 336.422\n",
      "Accuracy test inputs: 63.470790378006875 %%\n",
      "[37] loss: 400.326\n",
      "Accuracy test inputs: 60.99656357388316 %%\n",
      "[38] loss: 451.775\n",
      "Accuracy test inputs: 59.175257731958766 %%\n",
      "[39] loss: 486.441\n",
      "Accuracy test inputs: 57.83505154639175 %%\n",
      "[40] loss: 499.153\n",
      "Accuracy test inputs: 57.86941580756014 %%\n",
      "[41] loss: 488.818\n",
      "Accuracy test inputs: 59.03780068728523 %%\n",
      "[42] loss: 454.853\n",
      "Accuracy test inputs: 60.99656357388316 %%\n",
      "[43] loss: 401.293\n",
      "Accuracy test inputs: 63.71134020618557 %%\n",
      "[44] loss: 335.111\n",
      "Accuracy test inputs: 66.80412371134021 %%\n",
      "[45] loss: 264.421\n",
      "Accuracy test inputs: 71.47766323024055 %%\n",
      "[46] loss: 199.499\n",
      "Accuracy test inputs: 75.4295532646048 %%\n",
      "[47] loss: 155.219\n",
      "Accuracy test inputs: 80.03436426116839 %%\n",
      "[48] loss: 147.871\n",
      "Accuracy test inputs: 81.68384879725086 %%\n",
      "[49] loss: 192.181\n",
      "Accuracy test inputs: 79.55326460481099 %%\n",
      "[50] loss: 282.271\n",
      "Accuracy test inputs: 76.87285223367698 %%\n"
     ]
    }
   ],
   "source": [
    "print([i.requires_grad for i in twonn.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "for e in range(50):\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (e + 1, running_loss ))\n",
    "    tacc = model_test(twonn, ptest_loader)\n",
    "    training_loss.append(running_loss )\n",
    "    test_acc.append(tacc)\n",
    "    running_loss = 0.0\n",
    "    #print(test_acc)\n",
    "    if test_acc[-1] > 85:\n",
    "        print(\"early stopping! Accuracy on test dataset > 90%\")\n",
    "        break\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(ptrain_loader, 0):\n",
    "        #print(type(i), type(data))\n",
    "        inputs, labels = data\n",
    "        #print(inputs.size(), labels.size())\n",
    "        #adam.zero_grad()\n",
    "        outputs = my_nn(inputs.float())\n",
    "        #print(outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        running_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(twonn.state_dict(), \"model/two_80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test inputs: 80.10309278350516 %%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.32      0.29       684\n",
      "           1       0.78      0.73      0.75      2226\n",
      "\n",
      "    accuracy                           0.63      2910\n",
      "   macro avg       0.52      0.52      0.52      2910\n",
      "weighted avg       0.65      0.63      0.64      2910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tacc = model_test(my_nn, test_loader)\n",
    "#print(tacc)\n",
    "from sklearn.metrics import classification_report\n",
    "allpredicted = []\n",
    "acc2 = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in ptest_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = twonn(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        allpredicted += predicted\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy test inputs: {} %%'.format(100 * correct / total))\n",
    "\n",
    "print(classification_report(allpredicted, ptesty))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
